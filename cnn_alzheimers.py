# -*- coding: utf-8 -*-
"""CNN_Alzheimers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1natw-tnuPgugz_6CkJM9q7mlzTEm-i2F
"""

#Importing Models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from lightgbm import LGBMClassifier
from xgboost.sklearn import XGBClassifier


import matplotlib.pyplot as plt
from matplotlib import pyplot
import numpy as np
import pandas as pd
import os
from sklearn.preprocessing import StandardScaler
from sklearn.metrics  import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import sklearn
from sklearn.model_selection import StratifiedKFold
print(sklearn.__version__)

from google.colab import drive
drive.mount('/content/gdrive')
datapath="/content/gdrive/My Drive"

df_alzheimer = pd.read_csv(os.path.join(datapath,'Alzheimer_Data.csv'))
df_alzheimer = df_alzheimer.drop(["SubjectID"], axis=1)

!pip install mrmr_selection
!import mrmr

from sklearn.model_selection import train_test_split
import lightgbm as lgb
from sklearn.metrics import precision_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import RFE
from sklearn.neural_network import MLPClassifier

import mrmr
from mrmr import mrmr_classif

from sklearn.model_selection import cross_val_score
import statistics
from sklearn.decomposition import PCA
import random
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import ExtraTreesClassifier

X = df_alzheimer.drop(["Diagnosis (0 - HC, 1 - MCI, 2 - AD)"], axis=1)
y = df_alzheimer["Diagnosis (0 - HC, 1 - MCI, 2 - AD)"]
X=(X-X.mean())/X.std()

#Add another standardization step here
scaler = StandardScaler().set_output(transform="pandas")
X = scaler.fit_transform(X)

'''selected_features = mrmr_classif(X=X, y=y, K=93)
X = X[selected_features]
print(selected_features)
X'''

from sklearn.model_selection import GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state = 8
)
scaled_X_train = scaler.fit_transform(X_train)

from tensorflow.keras import layers, models
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import GridSearchCV

acc_list = []

for i in range(1, 100):
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.30, random_state = i
  )
  scaled_X_train = scaler.fit_transform(X_train)
  model = Sequential()
  model.add(layers.Dense(403, activation='sigmoid', input_shape=(X_train.shape[1],)))
  model.add(layers.Dense(188, activation='tanh'))
  model.add(layers.Dense(233, activation='sigmoid'))
  model.add(layers.Dense(200, activation='relu'))
  model.add(layers.Dense(3, activation='softmax'))
  model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',  # Since the target labels are integers, not one-hot encoded
                      metrics=['accuracy'])
  model.fit(X_train, y_train, epochs=6, batch_size=8, validation_split=0.16)
   # Evaluate the model on the test data
  test_loss, test_accuracy = model.evaluate(X_test, y_test)
  acc_list.append(test_accuracy)
  print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print("Average Test Accuracy: ", sum(acc_list)/len(acc_list))

print(len(acc_list))
from statistics import mean, stdev
print(mean(acc_list))

